{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEtwCH3wYZoPNbSqFpDgtX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/torrhen/paper-transformer/blob/main/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import math"
      ],
      "metadata": {
        "id": "84MUgrjqwAvy"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "  '''\n",
        "  Scaled Dot-Product Attention function as described in section 3.2.1. Used as part of the Multi-Head Attention layer.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super(ScaledDotProductAttention, self).__init__()\n",
        "    # calculate attention weights\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "  def forward(self, Q, K, V, mask=None):\n",
        "    # transpose the final 2 dimensions of K to allow multiplication with Q\n",
        "    K = K.permute(0, 1, 3, 2) # [b, h, sz_k, d_k] -> [b, h, d_k, sz_k]\n",
        "\n",
        "    # calulate attention matrix between Q and K\n",
        "    attn = Q.matmul(K) # [b, h, sz_q, d_q] @ [b, h, d_k, sz_k] -> [b, h, sz_q, sz_k]\n",
        "\n",
        "    # scale attention matrix by factor sqrt(d_k)\n",
        "    attn = attn / torch.tensor(K.shape[-2])\n",
        "\n",
        "    # mask out illegal attention value connections\n",
        "    if mask is not None:\n",
        "      attn = attn.masked_fill_(mask, -math.inf)\n",
        "\n",
        "    # convert attention values to weights\n",
        "    attn = self.softmax(attn)\n",
        "    # multiply weighted attention with V\n",
        "    out = attn.matmul(V)\n",
        "\n",
        "    return out, attn # attention weighted values, attention weights\n"
      ],
      "metadata": {
        "id": "Rta-cUL5Us23"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  '''\n",
        "  Multi-Head Attention sub-layer as described in section 3.2.2. Used as part of the Encoder layer.\n",
        "  '''\n",
        "  def __init__(self, d_model, h):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    # embedding size\n",
        "    self.d_model = d_model\n",
        "    # number of heads\n",
        "    self.h = h\n",
        "    # embedding projection size for query, keys and values vectors\n",
        "    self.d_q = self.d_k = self.d_v = self.d_model // self.h\n",
        "    # linear projection layers for embeddings\n",
        "    self.fc_Q = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
        "    self.fc_K = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
        "    self.fc_V = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
        "    # attention function\n",
        "    self.attention = ScaledDotProductAttention()\n",
        "    # linear projection layer for attention\n",
        "    self.fc_mh_out = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
        "\n",
        "  def forward(self, Q, K, V, mask=None):\n",
        "    batch_size = Q.shape[0]\n",
        "    # linear projection of Q, K and V\n",
        "    p_Q = self.fc_Q(Q) # [b, sz_q, d_model] -> [b, sz_q, d_model]\n",
        "    p_K = self.fc_K(K) # [b, sz_k, d_model] -> [b, sz_k, d_model]\n",
        "    p_V = self.fc_V(V) # [b, sz_v, d_model] -> [b, sz_v, d_model]\n",
        "\n",
        "    # divide embedding dimension into seperate heads for Q, K, V\n",
        "    p_Q = p_Q.reshape((batch_size, -1, self.h, self.d_q)) # [b, sz_q, d_model] -> [b, sz_q, h, d_q]\n",
        "    p_K = p_K.reshape((batch_size, -1, self.h, self.d_k)) # [b, sz_k, d_model] -> [b, sz_k, h, d_k]\n",
        "    p_V = p_V.reshape((batch_size, -1, self.h, self.d_v)) # [b, sz_v, d_model] -> [b, sz_v, h, d_v]\n",
        "\n",
        "    # move the head dimension of Q, K and V\n",
        "    p_Q = p_Q.permute((0, 2, 1, 3)) # [b, sz_q, h, d_q] -> [b, h, sz_q, d_q]\n",
        "    p_K = p_K.permute((0, 2, 1, 3)) # [b, sz_k, h, d_k] -> [b, h, sz_k, d_k]\n",
        "    p_V = p_V.permute((0, 2, 1, 3)) # [b, sz_v, h, d_v] -> [b, h, sz_v, d_v]\n",
        "\n",
        "    # calculate the scaled dot product attention for each head in parallel\n",
        "    mh_out, mh_attn = self.attention(p_Q, p_K, p_V, mask)\n",
        "\n",
        "    # move the head dimension of the attention weighted values\n",
        "    mh_out = mh_out.permute((0, 2, 1, 3)) # [b, sz_v, h, d_v] -> [b, sz_v, h, d_v]\n",
        "\n",
        "    # concatenate heads of attention weighted values\n",
        "    mh_out = mh_out.reshape((batch_size, -1, self.d_model)) # [b, sz_v, h, d_v] -> [b, sz_v, h * d_v (d_model)]\n",
        "\n",
        "    # linear projection of attention weighted values\n",
        "    mh_out = self.fc_mh_out(mh_out) # [b, sz_v, d_model] -> [b, sz_v, d_model]\n",
        "\n",
        "    return mh_out, mh_attn # multi-head output, multi-head attention weights"
      ],
      "metadata": {
        "id": "77MHJRmNXCaS"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNetwork(nn.Module):\n",
        "  '''\n",
        "  Position-wise Feed Forward Network sub-layer as described in section 3.3. Used as part of the Encoder layer.\n",
        "  '''\n",
        "  def __init__(self, d_model, d_ff):\n",
        "    super(FeedForwardNetwork, self).__init__()\n",
        "    # input size\n",
        "    self.d_model = d_model\n",
        "    # hidden units\n",
        "    self.d_ff = d_ff\n",
        "    # feed forward network layers\n",
        "    self.fc_1 = nn.Linear(in_features=self.d_model, out_features=self.d_ff)\n",
        "    self.fc_2 = nn.Linear(in_features=self.d_ff, out_features=self.d_model)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.fc_2(self.relu(self.fc_1(x)))"
      ],
      "metadata": {
        "id": "Mlq1fCz36Az_"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  '''\n",
        "  Positional Encoding as described in section 3.5.\n",
        "  '''\n",
        "  def __init__(self, d_model):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    # embedding size\n",
        "    self.d_model = d_model\n",
        "    # 2i / d_model\n",
        "    self.exp = torch.arange(start=0, end=self.d_model, step=2, dtype=torch.float32) / self.d_model\n",
        "    # 10000\n",
        "    self.base = torch.full(size=(self.exp.shape[-1],), fill_value=10000.0, dtype=torch.float32)\n",
        "    # 10000 ^ (2i / d_model)\n",
        "    self.denominator = torch.pow(self.base, self.exp)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # input sequence size\n",
        "    sz_x = x.shape[-2]\n",
        "    # initialise positional encoding for each sequence position\n",
        "    pe = torch.zeros(size=(sz_x, self.d_model))\n",
        "    \n",
        "    # calculate positional encoding for each position in the input sequence\n",
        "    for pos in range(sz_x):\n",
        "      # PE(pos, 2i) = sin(pos / 10000^(2i / d_model))\n",
        "      pe[pos, 0::2] = torch.sin(self.denominator)\n",
        "      # PE(pos, 2i+1) = cos(pos / 10000^(2i / d_model))\n",
        "      pe[pos, 1::2] = torch.cos(self.denominator)\n",
        "\n",
        "    # combine input embedding and positional encoding\n",
        "    x = x + pe\n",
        "    return x"
      ],
      "metadata": {
        "id": "SAu1MdGs-_MK"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  '''\n",
        "  Encoder layer as described in section 3.1. Contains the multi-head attention and feed forward network sub-layers.\n",
        "  '''\n",
        "  def __init__(self, d_model, d_ff):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    # embedding size\n",
        "    self.d_model = d_model\n",
        "    # number of attention heads\n",
        "    self.h = 8\n",
        "    # feed foward network hidden units\n",
        "    self.d_ff = d_ff\n",
        "    # multi-head attention sub-layer\n",
        "    self.mha = MultiHeadAttention(self.d_model, self.h)\n",
        "    # multi-head attention layer norm\n",
        "    self.layer_norm_mha = nn.LayerNorm(normalized_shape=self.d_model)\n",
        "    # feed forward network sub-layer\n",
        "    self.ffn = FeedForwardNetwork(self.d_model, self.d_ff)\n",
        "    # feed foward network layer norm\n",
        "    self.layer_norm_ffn = nn.LayerNorm(normalized_shape=self.d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # multihead attention\n",
        "    query = keys = values = x\n",
        "    mha_out, mha_attn = self.mha(query, keys, values)\n",
        "    # residual connection and layer norm\n",
        "    x = self.layer_norm_mha(x + mha_out)\n",
        "\n",
        "    # feed forward network\n",
        "    ffn_out = self.ffn(x)\n",
        "    # residual connection and layer norm\n",
        "    x = self.layer_norm_ffn(x + ffn_out)\n",
        "    return x"
      ],
      "metadata": {
        "id": "OqaS7QHFwv2E"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  '''\n",
        "  Encoder as described in section 3.1. Contains multiple encoder layers.\n",
        "  '''\n",
        "  def __init__(self, N, d_model, h, d_ff):\n",
        "    super(Encoder, self).__init__()\n",
        "    # number of encoder layers\n",
        "    self.N = N\n",
        "    # embedding size\n",
        "    self.d_model = d_model\n",
        "    # number of attention heads\n",
        "    self.h = h\n",
        "    # feed forward network hidden units\n",
        "    self.d_ff = d_ff\n",
        "    # encoder of N encoder layers\n",
        "    self.encoder = nn.ModuleList([EncoderLayer(self.d_model, self.d_ff) for i in range(self.N)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    # pass input through each layer of the encoder\n",
        "    for encoder_layer in self.encoder:\n",
        "      x = encoder_layer(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "5kBC4ScR4UdR"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  '''\n",
        "  Decoder layer as described in section 3.1. Contains the multi-head attention and feed forward network sub-layers.\n",
        "  '''\n",
        "  def __init__(self, d_model, d_ff):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    # embedding size\n",
        "    self.d_model = d_model\n",
        "    # number of attention heads\n",
        "    self.h = 8\n",
        "    # feed foward network hidden units\n",
        "    self.d_ff = d_ff\n",
        "\n",
        "    # masked multi-head attention sub-layer\n",
        "    self.masked_mha = MultiHeadAttention(self.d_model, self.h)\n",
        "    # masked multi-head attention layer norm\n",
        "    self.layer_norm_masked_mha = nn.LayerNorm(normalized_shape=self.d_model)\n",
        "\n",
        "    # multi-head attention sub-layer\n",
        "    self.mha = MultiHeadAttention(self.d_model, self.h)\n",
        "    # multi-head attention layer norm\n",
        "    self.layer_norm_mha = nn.LayerNorm(normalized_shape=self.d_model)\n",
        "\n",
        "    # feed forward network sub-layer\n",
        "    self.ffn = FeedForwardNetwork(self.d_model, self.d_ff)\n",
        "    # feed foward network layer norm\n",
        "    self.layer_norm_ffn = nn.LayerNorm(normalized_shape=self.d_model)\n",
        "\n",
        "  def forward(self, x, encoder_output, mask=None):\n",
        "    # masked multi-head attention\n",
        "    query = keys = values = x\n",
        "    masked_mha_out, masked_mha_attn = self.masked_mha(query, keys, values, mask)\n",
        "    # residual connection and layer norm\n",
        "    x = self.layer_norm_masked_mha(x + masked_mha_out)\n",
        "\n",
        "    # multi-head attention\n",
        "    query = x\n",
        "    keys = values = encoder_output\n",
        "    mha_out, mha_attn = self.mha(query, keys, values)\n",
        "    # residual connection and layer norm\n",
        "    x = self.layer_norm_mha(x + mha_out)\n",
        "\n",
        "    # feed forward network\n",
        "    ffn_out = self.ffn(x)\n",
        "    # residual connection and layer norm\n",
        "    x = self.layer_norm_ffn(x + ffn_out)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "Jp2RgZvbR2tF"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  '''\n",
        "  Decoder as described in section 3.1. Contains multiple decoder layers.\n",
        "  '''\n",
        "  def __init__(self, N,  d_model, h, d_ff):\n",
        "    super(Decoder, self).__init__()\n",
        "    # number of decoder layers\n",
        "    self.N = N\n",
        "    # embedding size\n",
        "    self.d_model = d_model\n",
        "    # number of attention heads\n",
        "    self.h = h\n",
        "    # feed forward network hidden units\n",
        "    self.d_ff = d_ff\n",
        "    # decoder of N decoder layers\n",
        "    self.decoder = nn.ModuleList([DecoderLayer(self.d_model, self.d_ff) for i in range(self.N)])\n",
        "\n",
        "  def forward(self, x, encoder_output, mask=None):\n",
        "    # pass inputs through each layer of the decoder\n",
        "    for decoder_layer in self.decoder:\n",
        "      x = decoder_layer(x, encoder_output, mask)\n",
        "    return x"
      ],
      "metadata": {
        "id": "ewp38Gs5poMr"
      },
      "execution_count": 67,
      "outputs": []
    }
  ]
}