{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBQa22qEeMbdYOn2918dq1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/torrhen/deep-learning-papers/blob/main/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "84MUgrjqwAvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "  '''\n",
        "  Scaled Dot-Product Attention function as described in section 3.2.1. Used as part of the Multi-Head Attention layer.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super(ScaledDotProductAttention, self).__init__()\n",
        "    # calculate attention weights\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "  def forward(self, Q, K, V):\n",
        "    # transpose the final 2 dimensions of K to allow multiplication with Q\n",
        "    K = K.permute(0, 1, 3, 2) # [b, h, sz_k, d_k] -> [b, h, d_k, sz_k]\n",
        "\n",
        "    # calulate attention matrix between Q and K\n",
        "    attn = Q.matmul(K) # [b, h, sz_q, d_q] @ [b, h, d_k, sz_k] -> [b, h, sz_q, sz_k]\n",
        "\n",
        "    # scale attention matrix by factor sqrt(d_k)\n",
        "    attn = attn / torch.tensor(K.shape[-2])\n",
        "    # convert attention values to weights\n",
        "    attn = self.softmax(attn)\n",
        "    # multiply weighted attention with V\n",
        "    out = attn.matmul(V)\n",
        "\n",
        "    return out, attn # attention weighted values, attention weights\n"
      ],
      "metadata": {
        "id": "Rta-cUL5Us23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  '''\n",
        "  Multi-Head Attention sub-layer as described in section 3.2.2. Used as part of the Encoder layer.\n",
        "\n",
        "  TODO: mask, batch dimension\n",
        "\n",
        "  '''\n",
        "  def __init__(self, d_model, h):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    # embedding size\n",
        "    self.d_model = d_model\n",
        "    # number of heads\n",
        "    self.h = h\n",
        "    # embedding projection size for query, keys and values vectors\n",
        "    self.d_q = self.d_k = self.d_v = self.d_model // self.h\n",
        "    # linear projection layers for embeddings\n",
        "    self.fc_Q = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
        "    self.fc_K = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
        "    self.fc_V = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
        "    # attention function\n",
        "    self.attention = ScaledDotProductAttention()\n",
        "    # linear projection layer for attention\n",
        "    self.fc_mh_out = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
        "\n",
        "  def forward(self, Q, K, V):\n",
        "    # linear projection of Q, K and V\n",
        "    p_Q = self.fc_Q(Q) # [b, sz_q, d_model] -> [b, sz_q, d_model]\n",
        "    p_K = self.fc_K(K) # [b, sz_k, d_model] -> [b, sz_k, d_model]\n",
        "    p_V = self.fc_V(V) # [b, sz_v, d_model] -> [b, sz_v, d_model]\n",
        "\n",
        "    # divide embedding dimension into seperate heads for Q, K, V\n",
        "    p_Q = p_Q.reshape((1, -1, self.h, self.d_q)) # [b, sz_q, d_model] -> [b, sz_q, h, d_q]\n",
        "    p_K = p_K.reshape((1, -1, self.h, self.d_k)) # [b, sz_k, d_model] -> [b, sz_k, h, d_k]\n",
        "    p_V = p_V.reshape((1, -1, self.h, self.d_v)) # [b, sz_v, d_model] -> [b, sz_v, h, d_v]\n",
        "\n",
        "    # move the head dimension of Q, K and V\n",
        "    p_Q = p_Q.permute((0, 2, 1, 3)) # [b, sz_q, h, d_q] -> [b, h, sz_q, d_q]\n",
        "    p_K = p_K.permute((0, 2, 1, 3)) # [b, sz_k, h, d_k] -> [b, h, sz_k, d_k]\n",
        "    p_V = p_V.permute((0, 2, 1, 3)) # [b, sz_v, h, d_v] -> [b, h, sz_v, d_v]\n",
        "\n",
        "    # calculate the scaled dot product attention for each head in parallel\n",
        "    mh_out, mh_attn = self.attention(p_Q, p_K, p_V)\n",
        "\n",
        "    # move the head dimension of the attention weighted values\n",
        "    mh_out = mh_out.permute((0, 2, 1, 3)) # [b, sz_v, h, d_v] -> [b, sz_v, h, d_v]\n",
        "\n",
        "    # concatenate heads of attention weighted values\n",
        "    mh_out = mh_out.reshape((1, -1, self.d_model)) # [b, sz_v, h, d_v] -> [b, sz_v, h * d_v (d_model)]\n",
        "\n",
        "    # linear projection of attention weighted values\n",
        "    mh_out = self.fc_mh_out(mh_out) # [b, sz_v, d_model] -> [b, sz_v, d_model]\n",
        "\n",
        "    return mh_out, mh_attn # multi-head output, multi-head attention weights"
      ],
      "metadata": {
        "id": "77MHJRmNXCaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(10)\n",
        "\n",
        "batch_size = 1\n",
        "# sizes of Q, K and V\n",
        "sz_q = sz_k = sz_v = 10\n",
        "# embedding dim\n",
        "d_model = 512\n",
        "\n",
        "# query\n",
        "Q = torch.randn(size=(batch_size, sz_q, d_model), dtype=torch.float32) # [b, sz_q, d_model]\n",
        "# keys\n",
        "K = torch.randn(size=(batch_size, sz_k, d_model), dtype=torch.float32) # [b, sz_k, d_model]\n",
        "# values\n",
        "V = torch.randn(size=(batch_size, sz_v, d_model), dtype=torch.float32) # [b, sz_v, d_model]"
      ],
      "metadata": {
        "id": "nEVr8oHrhYR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test multi-head attention layer\n",
        "multihead_attention = MultiHeadAttention(d_model, 8)\n",
        "mh_out, mh_attn = multihead_attention(Q, K, V)\n",
        "\n",
        "print(mh_out.shape)\n",
        "print(mh_attn.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mz1qPwkmhiFm",
        "outputId": "1d58f9d7-6e44-4274-c965-c1945fa7b550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 512])\n",
            "torch.Size([1, 8, 10, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNetwork(nn.Module):\n",
        "  '''\n",
        "  Position-wise Feed Forward Network sub-layer as described in section 3.3. Used as part of the Encoder layer.\n",
        "  '''\n",
        "  def __init__(self, d_model, d_ff):\n",
        "    super(FeedForwardNetwork, self).__init__()\n",
        "    # input size\n",
        "    self.d_model = d_model\n",
        "    # hidden units\n",
        "    self.d_ff = d_ff\n",
        "    # feed forward network layers\n",
        "    self.fc_1 = nn.Linear(in_features=d_model, out_features=d_ff)\n",
        "    self.fc_2 = nn.linear(in_features=d_ff, out_feature=d_model)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.fc_2(self.relu(self.fc_1(x)))"
      ],
      "metadata": {
        "id": "Mlq1fCz36Az_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  '''\n",
        "  Positional Encoding as described in section 3.5\n",
        "  '''\n",
        "  def __init__(self, d_model):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    # embedding size\n",
        "    self.d_model = d_model\n",
        "    # 2i / d_model\n",
        "    self.exp = torch.arange(start=0, end=self.d_model, step=2, dtype=torch.float32) / self.d_model\n",
        "    # 10000\n",
        "    self.base = torch.full(size=(self.exp.shape[-1],), fill_value=10000.0, dtype=torch.float32)\n",
        "    # 10000 ^ (2i / d_model)\n",
        "    self.denominator = torch.pow(self.base, self.exp)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # input sequence size\n",
        "    sz_x = x.shape[-2]\n",
        "    # initialise positional encoding for each sequence position\n",
        "    pe = torch.zeros(size=(sz_x, self.d_model))\n",
        "    \n",
        "    # calculate positional encoding for each position in the input sequence\n",
        "    for pos in range(sz_x):\n",
        "      # PE(pos, 2i) = sin(pos / 10000^(2i / d_model))\n",
        "      pe[pos, 0::2] = torch.sin(self.denominator)\n",
        "      # PE(pos, 2i+1) = cos(pos / 10000^(2i / d_model))\n",
        "      pe[pos, 1::2] = torch.cos(self.denominator)\n",
        "\n",
        "    # combine input embedding and positional encoding\n",
        "    x = x + pe\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "p = PositionalEncoding(512)\n",
        "\n",
        "output = p(torch.randn(3, 8, d_model))\n",
        "print(output)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAu1MdGs-_MK",
        "outputId": "38b7f061-086e-4b23-efdf-220f8609ac91"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.1409,  0.1681,  0.7124,  ..., -0.1476,  0.8687,  1.7650],\n",
            "         [ 2.2070,  0.6970,  2.7329,  ...,  1.7209,  2.6870, -1.0465],\n",
            "         [ 2.1869, -0.5471,  1.7622,  ...,  2.7226,  0.9175, -0.3200],\n",
            "         ...,\n",
            "         [ 1.4383,  0.6669,  0.8037,  ...,  0.5861,  2.2831, -0.4902],\n",
            "         [ 1.0945,  0.6509,  1.4815,  ...,  1.7653, -0.6370, -0.0856],\n",
            "         [ 0.9465, -0.3051,  2.0486,  ...,  0.2450,  1.3773, -0.5689]],\n",
            "\n",
            "        [[-0.3004,  1.5436,  0.4095,  ...,  1.0171, -0.1957,  0.7254],\n",
            "         [ 0.5467,  0.9954,  0.3952,  ...,  0.1653,  0.0851, -1.7218],\n",
            "         [ 1.6440,  0.6102,  1.3465,  ...,  1.1019,  0.4261, -0.0400],\n",
            "         ...,\n",
            "         [ 0.0062,  1.7500,  1.0579,  ...,  1.1991,  0.7634, -0.2441],\n",
            "         [ 2.1763,  1.8964, -0.9343,  ...,  0.7227,  2.0685,  0.2015],\n",
            "         [ 1.3125,  0.6929, -0.0996,  ...,  0.3745,  1.0795, -0.3656]],\n",
            "\n",
            "        [[ 1.0870,  0.6678,  0.8443,  ..., -0.2066, -0.3458, -0.0820],\n",
            "         [ 1.1817,  0.3142,  0.3208,  ..., -0.4793, -0.0497, -0.4920],\n",
            "         [ 1.3144, -0.9184, -0.1338,  ...,  0.3419,  1.4136, -0.2992],\n",
            "         ...,\n",
            "         [ 1.3995,  2.6728,  1.5917,  ...,  1.3597,  2.0066, -0.6939],\n",
            "         [ 0.2865,  0.6405,  0.1093,  ...,  1.5765, -0.8246,  0.8343],\n",
            "         [ 0.9555, -0.5295,  2.5100,  ...,  2.6097,  2.4560, -1.4061]]])\n",
            "torch.Size([3, 8, 512])\n"
          ]
        }
      ]
    }
  ]
}