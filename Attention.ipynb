{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORYks/4vHLG3tz8z2nYP3e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/torrhen/papers/blob/main/Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "84MUgrjqwAvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "  '''\n",
        "  Scaled Dot-Product Attention function as described in section 3.2.1. Used as part of the Multi-Head Attention layer.\n",
        "\n",
        "  TODO: attention dropout\n",
        "\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super(ScaledDotProductAttention, self).__init__()\n",
        "    # calculate attention weights\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "  def forward(self, Q, K, V):\n",
        "    # transpose the final 2 dimensions of K to allow multiplication with Q\n",
        "    K = K.permute(0, 1, 3, 2) # [b, h, sz_k, d_k] -> [b, h, d_k, sz_k]\n",
        "\n",
        "    # calulate attention matrix between Q and K\n",
        "    attn = Q.matmul(K) # [b, h, sz_q, d_q] @ [b, h, d_k, sz_k] -> [b, h, sz_q, sz_k]\n",
        "\n",
        "    # scale attention matrix by factor sqrt(d_k)\n",
        "    attn = attn / torch.tensor(K.shape[-2])\n",
        "    # convert attention values to weights\n",
        "    attn = self.softmax(attn)\n",
        "    # multiply weighted attention with V\n",
        "    out = attn.matmul(V)\n",
        "\n",
        "    return out, attn # attention weighted values, attention weights\n"
      ],
      "metadata": {
        "id": "Rta-cUL5Us23"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  '''\n",
        "  Multi-Head Attention function as described in section 3.2.2\n",
        "\n",
        "  TODO: attention dropout, mask, batch dimension\n",
        "\n",
        "  '''\n",
        "  def __init__(self, d_model, h):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    # embedding size\n",
        "    self.d_model = d_model\n",
        "    # number of heads\n",
        "    self.h = h\n",
        "    # embedding projection size for query, keys and values vectors\n",
        "    self.d_q = self.d_k = self.d_v = self.d_model // self.h\n",
        "    # linear projection layers for embeddings\n",
        "    self.fc_Q = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
        "    self.fc_K = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
        "    self.fc_V = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
        "    # attention function\n",
        "    self.attention = ScaledDotProductAttention()\n",
        "    # linear projection layer for attention\n",
        "    self.fc_mh_out = nn.Linear(in_features=self.d_model, out_features=self.d_model)\n",
        "\n",
        "  def forward(self, Q, K, V):\n",
        "    # linear projection of Q, K and V\n",
        "    p_Q = self.fc_Q(Q) # [b, sz_q, d_model] -> [b, sz_q, d_model]\n",
        "    p_K = self.fc_K(K) # [b, sz_k, d_model] -> [b, sz_k, d_model]\n",
        "    p_V = self.fc_V(V) # [b, sz_v, d_model] -> [b, sz_v, d_model]\n",
        "\n",
        "    # divide embedding dimension into seperate heads for Q, K, V\n",
        "    p_Q = p_Q.reshape((1, -1, self.h, self.d_q)) # [b, sz_q, d_model] -> [b, sz_q, h, d_q]\n",
        "    p_K = p_K.reshape((1, -1, self.h, self.d_k)) # [b, sz_k, d_model] -> [b, sz_k, h, d_k]\n",
        "    p_V = p_V.reshape((1, -1, self.h, self.d_v)) # [b, sz_v, d_model] -> [b, sz_v, h, d_v]\n",
        "\n",
        "    # move the head dimension of Q, K and V\n",
        "    p_Q = p_Q.permute((0, 2, 1, 3)) # [b, sz_q, h, d_q] -> [b, h, sz_q, d_q]\n",
        "    p_K = p_K.permute((0, 2, 1, 3)) # [b, sz_k, h, d_k] -> [b, h, sz_k, d_k]\n",
        "    p_V = p_V.permute((0, 2, 1, 3)) # [b, sz_v, h, d_v] -> [b, h, sz_v, d_v]\n",
        "\n",
        "    # calculate the scaled dot product attention for each head in parallel\n",
        "    mh_out, mh_attn = self.attention(p_Q, p_K, p_V)\n",
        "\n",
        "    # move the head dimension of the attention weighted values\n",
        "    mh_out = mh_out.permute((0, 2, 1, 3)) # [b, sz_v, h, d_v] -> [b, sz_v, h, d_v]\n",
        "\n",
        "    # concatenate heads of attention weighted values\n",
        "    mh_out = mh_out.reshape((1, -1, self.d_model)) # [b, sz_v, h, d_v] -> [b, sz_v, h * d_v (d_model)]\n",
        "\n",
        "    # linear projection of attention weighted values\n",
        "    mh_out = self.fc_mh_out(mh_out) # [b, sz_v, d_model] -> [b, sz_v, d_model]\n",
        "\n",
        "    return mh_out, mh_attn # multi-head output, multi-head attention weights"
      ],
      "metadata": {
        "id": "77MHJRmNXCaS"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(10)\n",
        "\n",
        "batch_size = 1\n",
        "# sizes of Q, K and V\n",
        "sz_q = sz_k = sz_v = 10\n",
        "# embedding dim\n",
        "d_model = 512\n",
        "\n",
        "# query\n",
        "Q = torch.randn(size=(batch_size, sz_q, d_model), dtype=torch.float32) # [b, sz_q, d_model]\n",
        "# keys\n",
        "K = torch.randn(size=(batch_size, sz_k, d_model), dtype=torch.float32) # [b, sz_k, d_model]\n",
        "# values\n",
        "V = torch.randn(size=(batch_size, sz_v, d_model), dtype=torch.float32) # [b, sz_v, d_model]"
      ],
      "metadata": {
        "id": "nEVr8oHrhYR_"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test multi-head attention layer\n",
        "multihead_attention = MultiHeadAttention(d_model, 8)\n",
        "mh_out, mh_attn = multihead_attention(Q, K, V)\n",
        "\n",
        "print(mh_out.shape)\n",
        "print(mh_attn.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mz1qPwkmhiFm",
        "outputId": "19033e78-e7d8-40bc-8ef8-fb70928f3dbf"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 512])\n",
            "torch.Size([1, 8, 10, 10])\n"
          ]
        }
      ]
    }
  ]
}